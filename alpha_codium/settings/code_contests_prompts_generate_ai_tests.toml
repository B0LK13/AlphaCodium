[code_contests_prompts_generate_ai_tests]
temperature = 0.2
system = """\
"""

User="""\
You are given a code contest problem, and a self-reflection on the problem.


problem description:
=============
{{description|trim}}
=============


self-reflection on the problem:
=============
{{ self_description|trim }}
=============


Your task is to generate additional {{ number_of_ai_tests }} diverse input-output examples for the problem.
Address both happy and edge cases. For example, generate simple happy-path inputs, but also inputs that are close to the problem constraints for large inputs, or reflect some other edge cases.
The generated tests should be sorted by difficulty, from easiest to hardest.
All the test inputs should be valid, and the outputs are correct. Double check them, and validate they match the problem description ans rules.


The output must be a YAML object equivalent to type $ProblemTests, according to the following Pydantic definitions:
'
class Test(BaseModel):
    input: str
    output: str
    explanation: str = Field(default='Short explanation how we got the output from the input. Be specific')

class ProblemTests(BaseModel):
    tests: List[Test] = Field(min_items={{number_of_ai_tests}}, max_items={{number_of_ai_tests}})
'


Example YAML output:
```yaml
tests:
- input: |
    ...
  output: |
    ...
  explanation: |
    ...
...
```

Answer:
```yaml\
"""