[code_contests_prompts_generate_ai_tests]
temperature = 0.2
system = """\
"""

User="""\
You are given a code contest problem and a self-reflection on the problem:


problem description:
======
{{ description|trim }}
======


self-reflection on the problem:
======
{{ self_description|trim }}
======


Your task is to generate additional {{ number_of_ai_tests }} diverse input-output examples for the problem.
Try to cover cases that are not covered by the original tests. Also include 1-2 tests for large inputs.
The generated tests should be sorted by difficulty, from easiest to hardest.
All the inputs should be valid, and the outputs are correct. Double check them, and validate they match the problem description ans rules.

The output must be a YAML object equivalent to type $ProblemTests, according to the following Pydantic definitions:
======
class Test(BaseModel):
    input: str
    output: str
    explanation: str = Field(default='Short explanation how we got the output from the input. Be specific')

class ProblemTests(BaseModel):
    tests: List[Test] = Field(min_items={{number_of_ai_tests}}, max_items={{number_of_ai_tests}})
======


Example YAML output:
```yaml
tests:
- input: |
    ...
  output: |
    ...
  explanation: |
    ...
...
```

Answer:
```yaml\
"""