{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02caaa22-0bad-4a10-b97b-03fd33afc753",
   "metadata": {},
   "source": [
    "# code contests exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd77365-59ef-4209-b7a0-02b33bbf4746",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633b737-f780-4bad-829b-1bc933a0474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install jupysql\n",
    "!pip install pandas-profiling\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bdb58-4d75-4190-9639-09403d6cc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31035f4-7e1d-4004-85b9-637f8816c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append (\"..\")\n",
    "from importlib import reload\n",
    "from alpha_codium.code_contests.data import provider\n",
    "import matplotlib.pyplot as plt\n",
    "reload(provider)\n",
    "\n",
    "import duckdb\n",
    "from alpha_codium.code_contests.data.provider import CodeContestDataProvider\n",
    "\n",
    "connection = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45f95a-cb89-4fab-8061-526f3aeb5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" pretty print problem and solution\"\"\"\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from black import format_str, FileMode\n",
    "import re\n",
    "\n",
    "def format_examples(description):\n",
    "    # Check if 'Examples' section exists\n",
    "    if 'Examples' not in description:\n",
    "        return description\n",
    "    \n",
    "    # Split the description into main description and examples\n",
    "    main_description, examples_section = description.split('Examples', 1)\n",
    "    \n",
    "    # Extract all the inputs and outputs\n",
    "    inputs = re.findall(r'Input\\n\\n([\\s\\S]*?)(?:Output|$)', examples_section)\n",
    "    outputs = re.findall(r'Output\\n\\n([\\s\\S]*?)(?:Input|$)', examples_section)\n",
    "    \n",
    "    # Create markdown table\n",
    "    table = '### Examples\\n\\n| Input | Output |\\n|-------|--------|\\n'\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        table += f'| {inp.strip()} | {out.strip()} |\\n'\n",
    "    \n",
    "    # Combine main description and table\n",
    "    formatted_description = main_description + table\n",
    "    return formatted_description\n",
    "\n",
    "def render_problem_solution(problem_json, to_display=False):\n",
    "    # Extract data from JSON\n",
    "    name = problem_json.get('name', '')\n",
    "    description = format_examples(problem_json.get('description', ''))\n",
    "    solution = problem_json.get('solution', '')\n",
    "    \n",
    "    # Format the solution using black\n",
    "    formatted_solution = format_str(solution, mode=FileMode())\n",
    "    \n",
    "    # Construct the markdown string\n",
    "    markdown_str = f\"\"\"\n",
    "## {name}\n",
    "\n",
    "### description\n",
    "{description}\n",
    "\n",
    "### solution\n",
    "```python\n",
    "{formatted_solution}\n",
    "```\n",
    "\"\"\"\n",
    "    if to_display:\n",
    "        display(Markdown(markdown_str))\n",
    "\n",
    "    else:\n",
    "        return markdown_str\n",
    "    # Display using Markdown\n",
    "\n",
    "def render_problem_set(problems, to_display=True):\n",
    "    sep = '<hr style=\"height:5px;border-width:0;color:black;background-color:gray\">\\n'\n",
    "    markdown_str = f\"\"\"\n",
    "# Problem set: length = {len(problems)}\n",
    "{sep}\n",
    "{sep.join([render_problem_solution(problem, False) for problem in problems])}\n",
    "\"\"\"\n",
    "    if to_display:\n",
    "        display(Markdown(markdown_str))\n",
    "\n",
    "    else:\n",
    "        return markdown_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bd340-0b17-4edf-bad1-0f35596ade82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" basic plot\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_by_source(df, source_col, x_col, y_col, title = None):\n",
    "    sources = df[source_col].unique()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=len(sources), figsize=(12, 4 * len(sources)))\n",
    "\n",
    "    # Check if there's only one source (axes won't be an array in that case)\n",
    "    if len(sources) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, source in zip(axes, sources):\n",
    "        subset = df[df[source_col] == source]\n",
    "        ax.bar(subset[x_col].astype(str), subset[y_col], color='skyblue', edgecolor='black')\n",
    "        ax.set_title( f'{title or y_col} for {source_col}: {source}')\n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.set_xticks(subset[x_col])\n",
    "        ax.set_xticklabels(subset[x_col].astype(str), rotation=45)\n",
    "    fig.suptitle(title, fontsize=16, y=1.03)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec97c2-7df7-46a0-ab55-b0e6d1c2a42d",
   "metadata": {},
   "source": [
    "### Load the dataset and translate column codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc8504-72e4-4172-ad01-c215dfd888ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cc = CodeContestDataProvider()\n",
    "translated = cc.translate_references(cc.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb2e02-2f63-44cd-946e-a50f07d1a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.connect(translated)\n",
    "cc.query(\"SHOW ALL TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa34a1-b192-489a-b315-055a5f6c5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(translated['train'][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d96225-5001-42d9-9522-4eb5f786face",
   "metadata": {},
   "source": [
    "## Splits and sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6e730-2996-4a94-961d-1db9af4140a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = pd.Series({ds_split:len(ds_value) for ds_split, ds_value in translated.items()})\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba35d3-b091-461e-9d96-b2285665a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_distribution = []\n",
    "for split in translated.keys():\n",
    "    df = cc.query(f\"select source, count(*) as num_samples from code_contests_{split} group by source order by source asc\")\n",
    "    df['split']=split\n",
    "    split_distribution.append(df)\n",
    "\n",
    "distribution = pd.concat(split_distribution, ignore_index=True)\n",
    "\n",
    "plot_by_source(distribution, 'split', 'source', 'num_samples', \"Source distribution by dataset split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dc4f6-e17f-4565-b64b-791e1d8e8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flattened = translated['train'].flatten()\n",
    "for col_name in flattened.column_names:\n",
    "    new_name = col_name.replace(\".\", \"__\")\n",
    "    if not col_name == new_name:\n",
    "        flattened = flattened.rename_column(col_name,new_name )\n",
    "                      \n",
    "connection = cc.connection\n",
    "connection.register(f\"train_flattened\", flattened.data.table)\n",
    "schema = connection.query(\"show table train_flattened\").df()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 50)\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eea7e1-b519-49e5-ad2e-ef5d0362dc0a",
   "metadata": {},
   "source": [
    "## Solutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44d894-3af7-425a-a911-5605bbd31dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" distribution of tests per source \"\"\" \n",
    "\n",
    "def bucketized_group_by(table_name, field, measure):\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        {field}, \n",
    "        CASE\n",
    "            WHEN counter =0  THEN '0'\n",
    "            WHEN counter BETWEEN 1 AND 10 THEN '1-10'\n",
    "            WHEN counter BETWEEN 11 AND 50 THEN '11-50'\n",
    "            WHEN counter BETWEEN 51 AND 100 THEN '51-100'\n",
    "            WHEN counter BETWEEN 101 AND 500 THEN '101-500'\n",
    "            WHEN counter BETWEEN 501 AND 1000 THEN '501-1000'\n",
    "        END AS bucket_range,\n",
    "        COUNT(*) AS counter_bucket\n",
    "    FROM\n",
    "        (select {field}, {measure} as counter from {table_name}) as temp\n",
    "    GROUP BY\n",
    "        source,\n",
    "        CASE\n",
    "            WHEN counter = 0 THEN '0'\n",
    "            WHEN counter BETWEEN 1 AND 10 THEN '1-10'\n",
    "            WHEN counter BETWEEN 11 AND 50 THEN '11-50'\n",
    "            WHEN counter BETWEEN 51 AND 100 THEN '51-100'\n",
    "            WHEN counter BETWEEN 101 AND 500 THEN '101-500'\n",
    "            WHEN counter BETWEEN 501 AND 1000 THEN '501-1000'\n",
    "        END\n",
    "    ORDER BY\n",
    "        source desc,\n",
    "            CASE\n",
    "            WHEN bucket_range = '0' THEN 0\n",
    "            WHEN bucket_range = '1-10' THEN 1\n",
    "            WHEN bucket_range = '11-50' THEN 11\n",
    "            WHEN bucket_range = '51-100' THEN 51\n",
    "            WHEN bucket_range = '101-500' THEN 101\n",
    "            WHEN bucket_range = '501-1000' THEN 501\n",
    "        END ASC;\n",
    "        \n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12477767-8964-461e-9259-be23086d8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_value = \"CODEFORCES\"  # Replace this with the desired value\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    name, \n",
    "    description, \n",
    "    UNNEST(solutions__language) as language, \n",
    "    UNNEST(solutions__solution) as solution \n",
    "FROM \n",
    "    train_flattened \n",
    "WHERE \n",
    "    source='{source_value}'\n",
    "\"\"\"\n",
    "\n",
    "solutions = connection.query(query)\n",
    "connection.register(f\"train_solutions\", solutions)\n",
    "solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903d820-b9f8-49cd-ba7c-ca19ea07f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_distribution = connection.query(f\"select '{source_value}' as source, language, count(*) as num_solutions from train_solutions group by language\").df()\n",
    "\n",
    "plot_by_source(language_distribution, 'source', 'language', 'num_solutions', \"Solution languages by source in training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f12998-4847-4bab-bc99-f327909d2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_sample = connection.query(\"select * from train_solutions USING SAMPLE 10%\")\n",
    "connection.register(f\"train_solutions_sample\", solutions_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02db10-3f88-4651-957e-b75aca4ab435",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_value = \"PYTHON3\"  # Replace this with the desired value\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    name, \n",
    "    count(*) as num_python_solutions_per_problem \n",
    "FROM \n",
    "    train_solutions_sample \n",
    "WHERE \n",
    "    language='{language_value}' \n",
    "GROUP BY \n",
    "    name\n",
    "\"\"\"\n",
    "\n",
    "solutions_per_problem = connection.query(query).df()\n",
    "connection.register(f\"train_solutions_per_problem\", solutions_per_problem)\n",
    "solutions_per_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f1269-e940-46ca-92fb-d8aeb341d77b",
   "metadata": {},
   "source": [
    "## Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa251ca-9ee5-43de-a75c-e6df0f7d9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    source,\n",
    "    len(public_tests__input) AS public_test_count,\n",
    "    len(private_tests__input) AS private_test_count,\n",
    "    len(generated_tests__input) AS generated_test_count\n",
    "FROM \n",
    "    train_flattened;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tests = connection.query(query)\n",
    "\n",
    "connection.register(f\"train_tests_per_problem\", tests)\n",
    "\n",
    "tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f138621-b0b2-4881-acd5-0bd0477771b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" distribution of tests per source \"\"\" \n",
    "def test_count_per_source(field):\n",
    "    test_count_distribution_query = bucketized_group_by('train_tests_per_problem', 'source', field )\n",
    "    return connection.query(test_count_distribution_query).df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052a3d8-6bf3-4dc2-b239-60114e2d20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_count_per_source('private_test_count')\n",
    "plot_by_source(df, 'source', 'bucket_range', 'counter_bucket', title = \"Distribution of private test counts in problems per source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d41b-9217-4c55-a296-2eda6cd7219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_count_per_source('generated_test_count')\n",
    "plot_by_source(df, 'source', 'bucket_range', 'counter_bucket', title = \"Distribution of generated test counts in problems per source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ab173-3a70-4cb8-a3d3-bc6a57664727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" public tests - examples provided with the problem text itself \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    source, \n",
    "    public_test_count, \n",
    "    COUNT(*) AS num_of_problems\n",
    "FROM \n",
    "    train_tests_per_problem\n",
    "GROUP BY \n",
    "    source, \n",
    "    public_test_count\n",
    "ORDER BY \n",
    "    source, \n",
    "    public_test_count;\n",
    "\n",
    "\"\"\"\n",
    "df =connection.query(query).df()\n",
    "\n",
    "plot_by_source(df, 'source', 'public_test_count', 'num_of_problems', \"Distribution of public (example) test counts in problems per source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d711b-9254-4638-ad4b-43bac42d2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.query(\"show all tables\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28ef60-e6a6-4d2d-8f28-dca41c8304ae",
   "metadata": {},
   "source": [
    "## Example visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c8455-e840-4b45-9fbf-22dea7817443",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = connection.query (\"select * from train_solutions_sample where language='PYTHON3' order by solution limit 10 \").df().to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82d2fe-5534-41dc-b9cd-e158266636b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem_set(examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
